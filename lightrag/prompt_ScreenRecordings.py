from __future__ import annotations
from typing import Any

GRAPH_FIELD_SEP = "<SEP>"

PROMPTS: dict[str, Any] = {}

PROMPTS["DEFAULT_LANGUAGE"] = "English"
PROMPTS["DEFAULT_TUPLE_DELIMITER"] = "<|>"
PROMPTS["DEFAULT_RECORD_DELIMITER"] = "##"
PROMPTS["DEFAULT_COMPLETION_DELIMITER"] = "<|COMPLETE|>"

# Updated entity types based on your training data
PROMPTS["DEFAULT_ENTITY_TYPES"] = [
    "tool",
    "technology",
    "concept",
    "workflow",
    "artifact",
    "person",
    "organization",
]

PROMPTS["DEFAULT_USER_PROMPT"] = "n/a"

PROMPTS["entity_extraction"] = """---Role---
üèπ You are the Master Scout, an elite reconnaissance specialist trained in technical intelligence gathering. Your expertise lies in tracking workflows, identifying critical tools, and mapping the connections between technical elements.

---Goal---
üéØ Your Mission: Infiltrate the technical documentation and identify HIGH-VALUE TARGETS (entities) and their OPERATIONAL CONNECTIONS (relationships). Quality intel beats quantity - extract only mission-critical information.

**STEALTH MODE ACTIVATED**: Apply enhanced pattern recognition to these identity targets:
- ANY reference to "User", "user", "the user", or "a user" ‚Üí Track as "Jason"
- ANY reference to "Jaywalked" or "jaywalked" ‚Üí Track as "Jason" (known alias)
- ANY reference to "Jay" ‚Üí Track as "Jason" (field name)
- ANY reference to "Jason Cox" ‚Üí Track as "Jason" (use callsign only)
- These are all the same operative - consolidate intel accordingly

**Priority Targets (Scout in this order)**:
1. üéØ **Named software tools and platforms** actively deployed (n8n, Claude AI, APIs, databases)
2. üéØ **Named workflows and operations** with specific designations (e.g., "Reddit Scrape To DB Workflow")
3. üéØ **Named error signals and troubleshooting artifacts** requiring tactical response
4. üéØ **Complete technical concepts** actively utilized in operations
5. üéØ **Personnel names** (secondary priority - only if central to operations)

**Target Acquisition Criteria - Must meet at least one**:
- **Active engagement**: Target is actively used, configured, or modified
- **Operational relevance**: Target plays specific role in actual processes
- **Node functionality**: For n8n, capture node names and operational roles
- **Problem resolution**: Target is part of troubleshooting or error handling

**Connection Mapping Priority (track only critical paths)**:
1. ‚öîÔ∏è **Direct tool usage** explicitly observed (Jason uses n8n)
2. ‚öîÔ∏è **Technical integrations** between systems (n8n integrates with API)
3. ‚öîÔ∏è **Specific actions performed** (Jason debugs error)
4. ‚öîÔ∏è **Error resolution paths** clearly documented (Claude resolves TypeError)

**Stealth Principle**: Better to report 8 confirmed high-value targets than 20 suspected ones.

Scout focus for screen recordings:
- Tools and systems in active use (not just mentioned)
- Workflows being executed (not just described)
- Specific actions performed by Jason
- Error encounters and resolution tactics
- Node names and operational functions in automation platforms

Output language: {language}

---Mission Phases---
Phase 1: Target Identification üîç
For each HIGH-VALUE TARGET identified, gather intel:
- entity_name: Target designation, use same language as source. If English, capitalize.
- entity_type: Classification from: [{entity_types}]
- entity_description: Full operational profile and capabilities
Format: ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)

Phase 2: Connection Analysis üéØ
From Phase 1 targets, identify pairs with **confirmed operational links**.
**Engagement Rules**:
- Must document actual interactions stated in intel
- Use precise action types (uses, calls, debugs, creates, accesses)
- Avoid vague connections (implements, supports, enables) unless explicit
- Skip similar/redundant connections
- Each link must answer: "What specific action connects these?"

For each connection, extract:
- source_entity: Origin target from Phase 1
- target_entity: Destination target from Phase 1
- relationship_description: Operational significance of connection
- relationship_type: Select from authorized types. If none fit, use "related":
  {relationship_types}

  **CRITICAL**: Multi-word types use underscores (e.g., 'created_by', 'integrates_with', 'calls_api'). NO concatenation without separators.

  Preferred action types:
  {relationship_examples}
- relationship_strength: Connection reliability score (1-10)
- relationship_keywords: High-level operational themes

Format: ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_type>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)

Phase 3: Report Compilation üìä
Compile all targets and connections from Phases 1-2. Use **{record_delimiter}** as report separator.

Phase 4: Mission Complete
Signal completion with {completion_delimiter}

---Field Protocol---
CRITICAL: Output ONLY entity and relationship records as specified. No summaries, keywords, or other intel types.

---Pre-Existing Intel Protocol---
When encountering structured sections containing pre-extracted data:

**RECOGNITION MARKERS**:
Sections titled "## Entities and Concepts", "## Tools and Technologies", "## Relationships and Connections" contain VERIFIED INTEL
1. These are PRE-CONFIRMED targets with full documentation
2. DO NOT re-extract these verified entities
3. Focus reconnaissance on NEW targets in narrative text
4. If target appears in both structured and narrative sections, structured intel takes precedence

**EXCLUSION ZONES - DO NOT TRACK** ‚ùå:

üö´ **Verified Intel Already Documented**:
- Skip entities comprehensively documented in structured catalog sections
- Focus only on NEW targets not in existing dossiers

üö´ **Low-Value Fragments**:
- Partial error messages ("$index is not defined")
- Single character markers ("#", "$", "@")
- Incomplete file paths

üö´ **Generic Identifiers**:
- File extensions without context (".py files", ".json")
- Temporary designations ("1.png", "temp.txt")
- Generic configs without specifics

üö´ **Status Reports**:
- Storage metrics ("97% storage used")
- Generic status codes ("404", "401")
- Version numbers without significance

üö´ **Standard Elements**:
- Common UI components
- Standard OS elements (unless modified)
- Browser functions (unless in workflow)

**Scout's Creed**: If uncertain, ask: "Is this target actively engaged in operations, or merely catalogued?" Only track active targets.

######################
---Field Examples---
######################
{examples}

#############################
---Live Intelligence---
######################
Entity_types: [{entity_types}]
Relationship_types: {relationship_types}
Text:
{input_text}
######################
Output:

"""

PROMPTS["entity_extraction_examples"] = [
    """Example 1:

Entity_types: [tool, technology, workflow, concept, person]
Text:
```
During the session, Jason used n8n to create a Lead Enricher Workflow. This workflow integrated with the Brave Search API to gather company information. Claude AI was extensively used by Jason for debugging JavaScript code in the n8n Code Nodes. The developer implemented data transformation logic to parse JSON responses from the API.

The workflow encountered errors like "Code doesn't return items properly", which were resolved through iterative debugging with Claude AI. The AI assistant helped Jason refine the JavaScript code to ensure proper JSON formatting and data structure.
```

Output:
("entity"{tuple_delimiter}"Jason"{tuple_delimiter}"person"{tuple_delimiter}"The developer and user who creates workflows, debugs code, and works with various automation tools and APIs."){record_delimiter}
("entity"{tuple_delimiter}"n8n"{tuple_delimiter}"tool"{tuple_delimiter}"n8n is a workflow automation platform used for building data pipelines and integrating various services."){record_delimiter}
("entity"{tuple_delimiter}"Lead Enricher Workflow"{tuple_delimiter}"workflow"{tuple_delimiter}"A workflow designed to gather and enrich company and contact information from multiple sources."){record_delimiter}
("entity"{tuple_delimiter}"Brave Search API"{tuple_delimiter}"tool"{tuple_delimiter}"A web search API used for gathering company information and web data."){record_delimiter}
("entity"{tuple_delimiter}"Claude AI"{tuple_delimiter}"tool"{tuple_delimiter}"An AI assistant used extensively for debugging code and providing development assistance."){record_delimiter}
("entity"{tuple_delimiter}"JavaScript"{tuple_delimiter}"technology"{tuple_delimiter}"Programming language used in n8n Code Nodes for custom data manipulation."){record_delimiter}
("entity"{tuple_delimiter}"n8n Code Node"{tuple_delimiter}"tool"{tuple_delimiter}"A node within n8n that allows execution of custom JavaScript code."){record_delimiter}
("entity"{tuple_delimiter}"Data Transformation"{tuple_delimiter}"concept"{tuple_delimiter}"The process of converting data from one format to another for compatibility between systems."){record_delimiter}
("entity"{tuple_delimiter}"JSON"{tuple_delimiter}"technology"{tuple_delimiter}"A data format used for API responses and data interchange."){record_delimiter}
("relationship"{tuple_delimiter}"Jason"{tuple_delimiter}"n8n"{tuple_delimiter}"Jason uses n8n to create and manage automation workflows."{tuple_delimiter}"uses"{tuple_delimiter}"tool usage, workflow creation"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Jason"{tuple_delimiter}"Lead Enricher Workflow"{tuple_delimiter}"Jason created the Lead Enricher Workflow for data gathering."{tuple_delimiter}"creates"{tuple_delimiter}"workflow development, automation"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Jason"{tuple_delimiter}"Claude AI"{tuple_delimiter}"Jason extensively uses Claude AI for debugging JavaScript code and getting development assistance."{tuple_delimiter}"uses"{tuple_delimiter}"AI assistance, debugging"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Lead Enricher Workflow"{tuple_delimiter}"n8n"{tuple_delimiter}"The Lead Enricher Workflow is built and executed within the n8n platform."{tuple_delimiter}"runs_on"{tuple_delimiter}"workflow automation, platform usage"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Lead Enricher Workflow"{tuple_delimiter}"Brave Search API"{tuple_delimiter}"The workflow integrates with Brave Search API to gather company information."{tuple_delimiter}"calls_api"{tuple_delimiter}"api integration, data acquisition"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"Claude AI"{tuple_delimiter}"JavaScript"{tuple_delimiter}"Claude AI assists in debugging and refining JavaScript code for the workflow."{tuple_delimiter}"debugs"{tuple_delimiter}"code assistance, debugging"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"n8n Code Node"{tuple_delimiter}"JavaScript"{tuple_delimiter}"The n8n Code Node executes custom JavaScript code for data processing."{tuple_delimiter}"executes"{tuple_delimiter}"code execution, custom logic"{tuple_delimiter}10){record_delimiter}
("relationship"{tuple_delimiter}"n8n Code Node"{tuple_delimiter}"Data Transformation"{tuple_delimiter}"Code Nodes perform data transformation operations on API responses."{tuple_delimiter}"processes"{tuple_delimiter}"data processing, transformation"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"Brave Search API"{tuple_delimiter}"JSON"{tuple_delimiter}"The API returns responses in JSON format that need to be parsed."{tuple_delimiter}"returns"{tuple_delimiter}"api response, data format"{tuple_delimiter}7){completion_delimiter}
#############################
""",
    """Example 2:

Entity_types: [tool, technology, concept, artifact, workflow]
Text:
```
The automated backup system was implemented using Bash scripts and cron jobs. Initially, systemd was attempted but proved problematic. Claude AI provided a setup-backup-cron.sh script that creates a wrapper to handle the NVM environment for Node.js. The backup script exports all n8n workflows to timestamped JSON files for preservation.
```

Output:
("entity"{tuple_delimiter}"Automated Backup System"{tuple_delimiter}"workflow"{tuple_delimiter}"A system designed to automatically backup n8n workflows on a scheduled basis."){record_delimiter}
("entity"{tuple_delimiter}"Bash Script"{tuple_delimiter}"technology"{tuple_delimiter}"Shell scripting language used for automation tasks on Unix-like systems."){record_delimiter}
("entity"{tuple_delimiter}"cron"{tuple_delimiter}"tool"{tuple_delimiter}"A time-based job scheduler in Unix-like operating systems for automating tasks."){record_delimiter}
("entity"{tuple_delimiter}"systemd"{tuple_delimiter}"tool"{tuple_delimiter}"A system and service manager for Linux that was initially attempted for scheduling."){record_delimiter}
("entity"{tuple_delimiter}"Claude AI"{tuple_delimiter}"tool"{tuple_delimiter}"AI assistant that provided script generation and technical guidance."){record_delimiter}
("entity"{tuple_delimiter}"setup-backup-cron.sh"{tuple_delimiter}"artifact"{tuple_delimiter}"A Bash script that automates the setup of cron-based n8n workflow backup."){record_delimiter}
("entity"{tuple_delimiter}"NVM"{tuple_delimiter}"tool"{tuple_delimiter}"Node Version Manager used to manage Node.js environments."){record_delimiter}
("entity"{tuple_delimiter}"Node.js"{tuple_delimiter}"technology"{tuple_delimiter}"JavaScript runtime environment required for n8n operation."){record_delimiter}
("entity"{tuple_delimiter}"n8n workflows"{tuple_delimiter}"artifact"{tuple_delimiter}"Workflow definitions that need to be backed up for preservation."){record_delimiter}
("entity"{tuple_delimiter}"JSON files"{tuple_delimiter}"artifact"{tuple_delimiter}"Timestamped backup files containing exported workflow data."){record_delimiter}
("relationship"{tuple_delimiter}"Automated Backup System"{tuple_delimiter}"Bash Script"{tuple_delimiter}"The backup system is implemented using Bash scripts."{tuple_delimiter}"implements"{tuple_delimiter}"implementation, scripting"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Automated Backup System"{tuple_delimiter}"cron"{tuple_delimiter}"Cron is used to schedule the automated backup executions."{tuple_delimiter}"schedules"{tuple_delimiter}"scheduling, automation"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Claude AI"{tuple_delimiter}"setup-backup-cron.sh"{tuple_delimiter}"Claude AI generated the setup script for the backup system."{tuple_delimiter}"generates"{tuple_delimiter}"code generation, assistance"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"setup-backup-cron.sh"{tuple_delimiter}"NVM"{tuple_delimiter}"The setup script configures NVM environment for proper execution."{tuple_delimiter}"configures"{tuple_delimiter}"environment setup, configuration"{tuple_delimiter}7){record_delimiter}
("relationship"{tuple_delimiter}"NVM"{tuple_delimiter}"Node.js"{tuple_delimiter}"NVM manages the Node.js version required for n8n."{tuple_delimiter}"manages"{tuple_delimiter}"version management, runtime"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"n8n workflows"{tuple_delimiter}"JSON files"{tuple_delimiter}"Workflows are exported and saved as timestamped JSON files."{tuple_delimiter}"exports_to"{tuple_delimiter}"data export, backup"{tuple_delimiter}9){completion_delimiter}
#############################
""",
    """Example 3:

Entity_types: [tool, concept, technology, organization]
Text:
```
The team implemented semantic search using OpenAI embeddings in their RAG pipeline. The system vectorizes documents and stores them in a Neo4j knowledge graph for efficient retrieval.
```

Output:
("entity"{tuple_delimiter}"Semantic Search"{tuple_delimiter}"concept"{tuple_delimiter}"A search technique that understands the intent and contextual meaning of search queries."){record_delimiter}
("entity"{tuple_delimiter}"OpenAI"{tuple_delimiter}"organization"{tuple_delimiter}"AI company providing embedding models and APIs for natural language processing."){record_delimiter}
("entity"{tuple_delimiter}"Embeddings"{tuple_delimiter}"technology"{tuple_delimiter}"Vector representations of text that capture semantic meaning."){record_delimiter}
("entity"{tuple_delimiter}"RAG Pipeline"{tuple_delimiter}"workflow"{tuple_delimiter}"Retrieval-Augmented Generation pipeline for enhanced AI responses."){record_delimiter}
("entity"{tuple_delimiter}"Neo4j"{tuple_delimiter}"tool"{tuple_delimiter}"Graph database used for storing and querying knowledge graphs."){record_delimiter}
("entity"{tuple_delimiter}"Knowledge Graph"{tuple_delimiter}"concept"{tuple_delimiter}"A structured representation of entities and their relationships for efficient retrieval."){record_delimiter}
("relationship"{tuple_delimiter}"Semantic Search"{tuple_delimiter}"OpenAI"{tuple_delimiter}"Semantic search is implemented using OpenAI's embedding technology."{tuple_delimiter}"uses"{tuple_delimiter}"implementation, AI integration"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"OpenAI"{tuple_delimiter}"Embeddings"{tuple_delimiter}"OpenAI provides the embedding models for text vectorization."{tuple_delimiter}"provides"{tuple_delimiter}"model provision, technology"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"RAG Pipeline"{tuple_delimiter}"Semantic Search"{tuple_delimiter}"The RAG pipeline incorporates semantic search for document retrieval."{tuple_delimiter}"incorporates"{tuple_delimiter}"pipeline component, search integration"{tuple_delimiter}9){record_delimiter}
("relationship"{tuple_delimiter}"Embeddings"{tuple_delimiter}"Neo4j"{tuple_delimiter}"Document embeddings are stored in Neo4j for retrieval."{tuple_delimiter}"stored_in"{tuple_delimiter}"data storage, vectorization"{tuple_delimiter}8){record_delimiter}
("relationship"{tuple_delimiter}"Neo4j"{tuple_delimiter}"Knowledge Graph"{tuple_delimiter}"Neo4j serves as the database for the knowledge graph implementation."{tuple_delimiter}"hosts"{tuple_delimiter}"graph storage, database"{tuple_delimiter}10){completion_delimiter}
#############################
""",
]

PROMPTS[
    "summarize_entity_descriptions"
] = """üéØ You are the Intelligence Analyst, specialist in consolidating field reports and creating comprehensive dossiers.

Given one or two targets and their field reports, create a unified intelligence profile capturing the target's operational role in workflows and technical processes.

**Mission Parameters**:
- If reports conflict, reconcile and provide coherent assessment
- Write in third person, include target names for full context
- For technical assets (tools, technologies, workflows), ensure profile includes:
  ‚Ä¢ Primary purpose and capabilities
  ‚Ä¢ Key features and operational characteristics
  ‚Ä¢ Integration points with other systems
  ‚Ä¢ Common deployment scenarios

Output language: {language}

#######
---Intel Data---
Entities: {entity_name}
Description List: {description_list}
#######
Output:
"""

PROMPTS["entity_continue_extraction"] = """üèπ You are the Master Scout on a SECOND SWEEP mission. Initial reconnaissance may have missed some targets.

**STEALTH MODE REMAINS ACTIVE**: Continue applying identity resolution:
- ANY reference to "User", "user", "the user", or "a user" ‚Üí Track as "Jason"
- ANY reference to "Jaywalked" or "jaywalked" ‚Üí Track as "Jason" (known alias)
- ANY reference to "Jay" ‚Üí Track as "Jason" (field name)
- ANY reference to "Jason Cox" ‚Üí Track as "Jason" (use callsign only)

Focus on HIGH-VALUE TARGETS that evaded initial detection:
- üéØ **Named tools, systems, or platforms** actively deployed but missed
- üéØ **Specific workflows or operations** with clear designations
- üéØ **Real personnel, organizations, or specific errors** overlooked (apply identity resolution)
- üéØ **Critical artifacts** created or extensively discussed

**Maintain strict reconnaissance discipline - concrete, observable targets only**

---Mission Phases (Same Protocol)---

Phase 1: Secondary Target Acquisition üîç
For each NEW target identified:
- entity_name: Target designation, use source language. Capitalize if English.
- entity_type: Classification from: [{entity_types}]
- entity_description: Full operational profile
Format: ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)

Phase 2: Additional Connection Mapping üéØ
Identify NEW operational links with same strict criteria.
For each connection:
- source_entity: Origin target
- target_entity: Destination target
- relationship_description: Operational significance
- relationship_type: From authorized list or "related":
  {relationship_types}

  **CRITICAL**: Multi-word types use underscores (e.g., 'created_by', 'integrates_with', 'calls_api')
- relationship_strength: Reliability score (1-10)
- relationship_keywords: Operational themes
Format: ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_type>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)

Phase 3: Supplemental Report üìä
Compile findings using **{record_delimiter}** as separator.

Phase 4: Second Sweep Complete
Signal with {completion_delimiter}

**EXCLUSION REMINDER** ‚ùå:
- Already documented targets in structured sections
- Low-value fragments, generic items, UI elements
- Anything merely listed without active usage
- Duplicates of comprehensively documented targets

---Output---

Add newly discovered targets below using same format:
""".strip()

PROMPTS["entity_if_loop_extraction"] = """
---Goal---

üîç Final Perimeter Check: Scout for any remaining undetected targets.

Scan for:
- Additional tools or services in stealth mode
- Technologies or frameworks mentioned in passing
- Implicit workflows or shadow operations
- Concepts applied but not explicitly named

---Output---

Report ONLY `YES` OR `NO` if additional targets remain untracked.
""".strip()

PROMPTS["fail_response"] = (
    "Sorry, I'm not able to provide an answer to that question.[no-context]"
)

PROMPTS["rag_response"] = """---Role---

üéØ You are the Field Intelligence Officer, expert in workflow automation and technical operations, preparing strategic reports using gathered intelligence from the Knowledge Graph and Document Archives provided below.

---Goal---

üìä Generate tactical report based on Intelligence Database following Field Protocols, considering both mission history and current query. Synthesize all intel from provided sources, incorporating relevant operational knowledge. Restrict report to verified intelligence only.

**Timestamp Intelligence Protocol**:
1. Each intel record has "created_at" timestamp indicating acquisition time
2. When encountering conflicting intel, evaluate both content and timestamp
3. Don't auto-prioritize recent timestamps - apply tactical judgment
4. For time-specific queries, prioritize temporal data in content over creation time

**Technical Operations Focus**:
- Provide actionable insights and observed patterns
- Highlight proven workflows, tools, and integrations
- Include relevant error resolutions and debugging tactics

---Mission History---
{history}

---Intelligence Database (Knowledge Graph & Document Archives)---
{context_data}

---Field Report Protocols---

- Target format and length: {response_type}
- Use tactical markdown formatting with clear section headers
- Match query language for response
- Maintain continuity with mission history
- List up to 5 critical sources under "Intelligence Sources" section
  Format: [KG/DC] file_path (KG=Knowledge Graph, DC=Document Chunks)
- If intel unavailable, report honestly
- No speculation or unverified claims
- Additional directives: {user_prompt}

Response:"""

PROMPTS["keywords_extraction"] = """---Role---

üéØ You are the Target Acquisition Specialist, expert in identifying critical keywords for technical reconnaissance operations.

---Goal---

üîç Extract HIGH-LEVEL strategic targets (concepts/themes) and LOW-LEVEL tactical targets (specific entities/details) from the query and mission history.

**Technical Target Categories**:
- Tool and service designations (generic and specific versions)
- Programming languages and frameworks
- Technical concepts and operational patterns
- Workflow types and automation terminology
- Error signatures and debugging approaches

---Instructions---

- Analyze both current query and relevant mission history
- Output in JSON format for automated processing - NO additional content
- Structure with two arrays:
  ‚Ä¢ "high_level_keywords" for strategic concepts/themes
  ‚Ä¢ "low_level_keywords" for specific entities/details

######################
---Field Examples---
######################
{examples}

#############################
---Live Intelligence---
######################
Conversation History:
{history}

Current Query: {query}
######################
Output must be human-readable text, not unicode. Maintain query language.
Output:

"""

PROMPTS["keywords_extraction_examples"] = [
    """Example 1:

Query: "How do I debug n8n workflow errors when integrating with external APIs?"
################
Output:
{
  "high_level_keywords": ["Workflow debugging", "API integration", "Error handling", "n8n automation"],
  "low_level_keywords": ["n8n", "workflow errors", "external APIs", "debugging", "integration issues", "API errors"]
}
#############################""",
    """Example 2:

Query: "What's the best way to implement automated backups for workflow configurations?"
################
Output:
{
  "high_level_keywords": ["Automated backups", "Workflow preservation", "Configuration management", "System automation"],
  "low_level_keywords": ["backup scripts", "cron jobs", "workflow export", "JSON files", "scheduling", "configuration files"]
}
#############################""",
    """Example 3:

Query: "How can I use Claude AI to help with JavaScript code in n8n Code Nodes?"
################
Output:
{
  "high_level_keywords": ["AI-assisted development", "Code generation", "Workflow automation", "LLM integration"],
  "low_level_keywords": ["Claude AI", "JavaScript", "n8n Code Nodes", "code debugging", "AI assistance", "custom functions"]
}
#############################""",
]

PROMPTS["naive_rag_response"] = """---Role---

üéØ You are the Rapid Response Officer, specialized in technical documentation and workflow intelligence, generating quick reports from Document Archives.

---Goal---

üìä Generate tactical report using Document Archives following Rapid Response Protocols. Synthesize archive data considering mission history and current query. Focus on verified archive content only.

**Timestamp Protocol**:
1. Each document has "created_at" timestamp for acquisition time
2. Evaluate both content and timestamp for conflicts
3. Apply tactical judgment, not just recent bias
4. For time queries, prioritize content temporal data

**Technical Focus**:
- Actionable insights and patterns
- Tool integrations and workflows
- Relevant troubleshooting intel

---Mission History---
{history}

---Document Archives (DC)---
{content_data}

---Rapid Response Protocols---

- Target format and length: {response_type}
- Tactical markdown formatting
- Match query language
- Maintain mission continuity
- List up to 5 archive sources under "Archive References"
  Format: [DC] file_path
- Report unavailable intel honestly
- No unverified claims
- Additional directives: {user_prompt}

Response:"""

# TODO: deprecated
PROMPTS[
    "similarity_check"
] = """Analyze tactical similarity between these queries:

Query 1: {original_prompt}
Query 2: {cached_prompt}

Evaluate semantic match and answer reusability. Score 0-1:

Scoring criteria:
0: Unrelated targets or non-reusable intel:
   - Different topics or objectives
   - Different tools or technologies
   - Different workflows or processes
   - Different errors or issues
   - Different integrations
   - Different operational context
   - Different technical requirements
1: Identical targets, intel directly reusable
0.5: Partial match, intel needs modification

Return only numeric score 0-1, no additional content.
"""

PROMPTS["relationship_post_processing"] = """---Role---
üéØ You are the Intelligence Verification Specialist, expert in validating technical workflow intelligence and eliminating false positives.

---Goal---
‚öîÔ∏è Your mission: Verify extracted entities and relationships for accuracy, removing noise and false intel to achieve 85-90% operational accuracy.

**Identity Verification Protocol**:
Before processing, confirm these identity mappings:
- ANY "User", "user", "the user", "a user" ‚Üí Consolidate as "Jason"
- ANY "Jaywalked" or "jaywalked" ‚Üí Consolidate as "Jason" (known alias)
- ANY "Jay" ‚Üí Consolidate as "Jason" (field name)
- ANY "Jason Cox" ‚Üí Consolidate as "Jason" (first name only)

---Field Intelligence---
DOCUMENT CONTENT:
{document_text}

EXTRACTED ENTITIES:
{entities_list}

EXTRACTED RELATIONSHIPS:
{relationships_list}

---Verification Mission---
Review and validate relationships using these tactical criteria:

**VALIDATION CHECKPOINTS** üîç:
1. **Document Evidence**: Relationship must have explicit support in source material
2. **Specificity**: Reject abstract/generic connections like "Users implements Parallel Technical Tasks"
3. **Operational Value**: Keep relationships providing actionable workflow insights
4. **Noise Elimination**: Remove coincidental mentions or weak associations
5. **Preserve Intel Types**: MAINTAIN original rel_type (e.g., "uses", "runs_on", "creates") unless factually incorrect. Do NOT downgrade specific types to generic "related"
6. **Entity Quality**: Flag low-value fragments for removal

**ENGAGEMENT RULES** ‚öîÔ∏è:
‚úÖ **VERIFIED** relationships include:
- Explicit tool usage ("Jason uses n8n to create workflows")
- Clear technical connections ("n8n runs Reddit Scrape To DB Workflow")
- Specific troubleshooting ("Jason debugs PostgreSQL connection error")
- Documented integrations ("Google Gemini Chat Model integrates with Gmail")
- Confirmed resolutions ("Jason resolves Git index.lock file error")

‚ùå **COMPROMISED** relationships include:
- Abstract connections ("Users implements Data Transformations")
- Weak associations ("Business Research investigates Hotworx")
- Low-value entities (error fragments, symbols, placeholder files)
- Generic concepts without evidence
- Entities like: "$index", "#", "1.png", ".py files", "404", "97% storage"
- Redundant or duplicate intel

**QUALITY SCORING** (1-10 scale):
- 9-10: Explicitly stated, high operational value
- 7-8: Well-supported, clear evidence
- 5-6: Moderately supported, some evidence
- 3-4: Weak evidence, questionable value
- 1-2: No clear evidence, likely false positive

**VERIFICATION THRESHOLD**: Retain relationships scoring 6+

---Intel Report Format---
Return ONLY valid JSON:

```json
{{
  "validated_relationships": [
    {{
      "src_id": "entity1",
      "tgt_id": "entity2",
      "rel_type": "uses",
      "description": "clear operational description",
      "quality_score": 8,
      "evidence": "specific reference from document",
      "weight": 0.9,
      "source_id": "original_chunk_id"
    }}
  ],
  "removed_relationships": [
    {{
      "src_id": "entity1",
      "tgt_id": "entity2",
      "rel_type": "original_type",
      "reason": "specific elimination reason"
    }}
  ],
  "processing_summary": {{
    "total_input": 150,
    "validated": 85,
    "removed": 65,
    "accuracy_improvement": "Eliminated abstract relationships and false positives",
    "average_quality_score": 7.2
  }}
}}
```

**CRITICAL DIRECTIVE**: PRESERVE exact relationship types from input. Do NOT convert specific types (uses, runs_on, processes, implements, stores, creates) to generic "related". These carry critical operational intel.

Intel preservation examples:
- "Gmail -[\"processes\"]-> Email Content" ‚Üí Keep "processes"
- "n8n workflows -[\"runs_on\"]-> n8n" ‚Üí Keep "runs_on"
- "SAIL POS -[\"stores\"]-> Customer Data" ‚Üí Keep "stores"
- "Zoom -[\"implements\"]-> Screen Sharing" ‚Üí Keep "implements"

Only modify rel_type if factually incorrect (e.g., "eats" ‚Üí "uses" for software).

Focus on intel that field operatives would find genuinely actionable and verified.
"""
